versions:
  v0.1: Project Setup with PostgreSQL and Django
    questions:
      - What Django apps should be created initially?
      - What are the basic Django settings configurations needed?
      - What PostgreSQL database name and user credentials?
      - Should we use Django's default project structure or customize it?
      - What static files and media files setup is needed?
      - What basic templates structure should be created?
    answers:
      - Create 'accounts', 'questions', 'tests', 'sessions' apps initially
      - Basic settings: DEBUG=True, PostgreSQL config, static files, Bootstrap CDN
      - Database: 'smart_mcq_db', user: 'mcq_user', password: secure random string
      - Use Django's default structure with minor customizations for templates
      - Static: '/static/' URL, media: '/media/' for question images
      - Base template with Bootstrap, separate templates for each app
    industry_solution:
      - MOODLE ARCHITECTURE: Clear separation of concerns across apps
      - CORE MODELS: User → Profile → Enrollment → Course/Class → Test → TestAttempt → Answer
      - LMS PATTERN: Course-based enrollment system (like Canvas/Blackboard)
      - SCALABILITY: Design for multi-tenancy from start (organization_id in all models)
      - STATE MANAGEMENT: TestAttempt tracks all session state and progress
      - IMPLEMENTATION: Add organization field to all major models immediately
    tests:
      - Django project creates successfully with all specified apps
      - PostgreSQL connection establishes with configured credentials
      - Static files serve correctly from /static/ URL
      - Media files serve correctly from /media/ URL
      - Bootstrap CDN loads properly in base template
      - All apps (accounts, questions, tests, sessions) are properly configured
      - Database migrations run without errors
      - Admin interface is accessible
      - Debug mode works and shows detailed error pages
      - Base template renders with proper Bootstrap styling
      - Organization model created with required fields
      - All models include organization_id field for multi-tenancy
    notes:
      - ARCHITECTURE: Based on proven LMS patterns used by millions
      - FUTURE-PROOF: Multi-tenant ready from day one
      - INDUSTRY STANDARD: Enrollment-based access control

  v0.2: Authentication with Role-Based Access Control (Student & Teacher)
    questions:
      - Should we extend Django's User model or create a Profile model?
      - What fields are needed for Student and Teacher profiles?
      - How should role-based permissions be implemented?
      - What authentication views are needed (login, logout, register)?
      - Should registration be open or invite-only?
      - What are the different permission levels for each role?
    answers:
      - Create Profile model with OneToOne to User (keeps User model clean)
      - Profile fields: role (Student/Teacher), created_at, updated_at, is_active
      - Use Django Groups: 'Students' and 'Teachers' with custom permissions
      - Standard views: login, logout, register with role selection
      - Open registration for MVP, add invite-only later if needed
      - Teachers: CRUD questions/tests, view results; Students: take tests, view own results
    notes:
      - ALTERNATIVE: Consider custom User model with role field for v1.4+ optimization
      - Profile approach chosen for MVP to avoid User model complexity early on
      - Role-based access is central to app logic, evaluate custom User in future versions
    tests:
      - Login with valid credentials (student and teacher)
      - Login with invalid credentials shows error message (single, not duplicate)
      - Logout functionality works properly
      - Registration creates user with correct role and group
      - Role-based dashboard redirection works
      - Teacher and student can access their respective dashboards
      - Logged-in users cannot access login/register pages
      - Profile creation with organization assignment

  v0.3: Question Bank (CRUD for Teacher)
    questions:
      - What fields does a Question model need?
      - How many answer choices per question (fixed or variable)?
      - Should questions have categories/tags?
      - What question types are supported initially (only MCQ)?
      - Should questions have difficulty levels?
      - What bulk import/export features are needed?
      - How should question images be handled?
    answers:
      - Question: title, description, created_by, created_at, updated_at, is_active
      - Fixed 4 choices (A, B, C, D) for MVP simplicity
      - Simple category CharField (no complex tagging initially)
      - Only MCQ type for MVP, extend later
      - Simple difficulty: Easy/Medium/Hard CharField
      - No bulk import for MVP, manual entry only
      - Optional ImageField for question images, stored in media/questions/
    notes:
      - FUTURE: Convert category CharField to ManyToMany Tag model in v3.6
      - Current simple approach supports basic categorization for MVP
      - Tag system must be flexible for question pooling in advanced versions
    tests:
      - Teacher can create questions with title, description, category, difficulty
      - Question creation requires exactly 4 choices (A, B, C, D)
      - Only one choice can be marked as correct
      - Questions support image upload
      - Teacher can edit existing questions and choices
      - Teacher can delete questions (soft delete)
      - Question list shows all organization questions
      - Only teachers can access question management
      - Students cannot access question CRUD operations
      - Question form has helpful placeholders

  v0.4: Test Bank (Assemble test with selected questions)
    questions:
      - What fields does a Test model need?
      - How should questions be selected for a test?
      - Should tests have time limits?
      - What are the test configuration options?
      - How should test templates be managed?
      - Can questions be reordered in a test?
      - Should there be test categories?
    answers:
      - Test: title, description, created_by, time_limit_minutes, created_at, is_active
      - Manual selection via checkboxes in admin/form, ManyToMany to Question
      - Yes, time limit in minutes (default 60)
      - Basic config: title, description, time limit, question selection
      - No templates for MVP, each test is unique
      - Questions displayed in order added, no reordering for MVP
      - Simple category CharField like questions
    tests:
      - Teacher can create tests with title, description, time limit, category
      - Test creation allows selecting questions via checkboxes
      - "Select All" functionality works for question selection
      - Test list shows question count and time limit
      - Test detail view displays all selected questions with correct answers
      - Teacher can edit tests and modify question selection
      - Teacher can delete tests (soft delete)
      - Only teachers can access test management
      - Students cannot access test CRUD operations
      - Test bank accessible from teacher dashboard
      - JavaScript selection counter works properly

  v0.5: Test Session Scheduling (Schedule time + generate access code)
    questions:
      - What fields does a TestSession model need?
      - How should access codes be generated?
      - What date/time scheduling options are needed?
      - Should sessions have duration limits?
      - Can sessions be rescheduled?
      - How should timezone handling work?
      - What happens if a session expires?
    answers:
      - TestSession: test, access_code, start_time, created_by, is_active (end_time calculated automatically)
      - 6-digit random alphanumeric code using Python secrets module
      - Start datetime field only (teacher sets), end_time = start_time + test.time_limit_minutes
      - Session window duration = test time_limit_minutes (no separate duration field needed)
      - Simple edit form for rescheduling start_time only (teacher only)
      - Use Django's timezone-aware datetimes with UTC storage, display in user's local timezone
      - Timezone detection via JavaScript or user profile setting for proper display
      - Expired sessions show 'Session Expired' message, no access allowed
    tests:
      - TestSession model creates with all required fields (no separate end_time field)
      - Access code generation produces 6-digit alphanumeric codes
      - Start datetime field saves correctly with timezone awareness
      - End time calculated correctly: start_time + test.time_limit_minutes
      - Teacher can schedule sessions with proper date/time selection
      - Access codes are unique across all active sessions
      - Session rescheduling updates start_time only, end_time recalculates
      - Timezone handling works with UTC storage and local display
      - JavaScript timezone detection for proper user experience
      - Expired sessions block student access appropriately
      - Session creation form validates date/time inputs with timezone
      - Duration automatically inherited from test time_limit_minutes
      - Only teachers can create and manage test sessions
      - Session listing shows all scheduled sessions in user's timezone
      - Access code security prevents easy guessing
      - Session model maintains relationship to test and creator
      - Multiple sessions per test supported correctly
      - Session expiration logic prevents late access based on calculated end_time
    notes:
      - CRITICAL: Students never access Test directly, only via TestSession
      - Access control: TestSession.access_code is the ONLY student entry point
      - Test model should NOT have any public access methods or codes
      - This separation allows multiple sessions per test with different schedules

  v0.6: Student Test Access (Access code + Dashboard listing)
    questions:
      - How should students enter access codes?
      - What information should be shown on student dashboard?
      - Should there be test status indicators?
      - How should upcoming vs completed tests be displayed?
      - What actions can students perform on listed tests?
      - Should there be search/filter options?
    answers:
      - Simple text input field with 'Join Test' button on dashboard
      - Dashboard: test title, session time, status, score (if completed)
      - Status indicators: Upcoming (blue), Ongoing (green), Completed (gray)
      - Separate sections with Bootstrap cards for each status
      - Actions: 'Join Test' (upcoming/ongoing), 'View Results' (completed)
      - No search/filter for MVP, simple chronological listing
    tests:
      - Access code input field validates 6-character alphanumeric codes
      - Invalid access codes show appropriate error messages
      - Valid access codes redirect to test session
      - Test sessions outside time window show 'not available' message
      - Student dashboard displays upcoming tests with proper status
      - Ongoing tests show 'Join Test' button
      - Completed tests show 'View Results' button
      - Test cards display correct information (title, time, teacher)
      - Status indicators show correct colors (blue/green/gray)
      - Students cannot access expired test sessions
      - Access code is case-insensitive
      - Dashboard refreshes test status appropriately
    notes:
      - LOGICAL ISSUE: How do students know which tests they can take?
      - MISSING: Student enrollment/assignment to tests mechanism
      - FLOW GAP: Students need a way to discover available tests beyond access codes
      - CONSIDERATION: Add teacher ability to 'assign' tests to specific students

  v1.0: Test Attempt UI (One question at a time, Next/Prev)
    questions:
      - How should the test-taking interface look?
      - Should there be a question counter (1 of 10)?
      - How should answer options be displayed?
      - What happens when a student selects an answer?
      - Should there be a progress indicator?
      - How should navigation buttons be styled?
      - Should there be keyboard shortcuts?
    answers:
      - Clean Bootstrap card layout with question at top, options below
      - Yes, show 'Question X of Y' at top right
      - Radio buttons with Bootstrap styling, clear labels A, B, C, D
      - Selection is saved immediately via AJAX (no page refresh)
      - Simple progress bar at bottom showing completed questions
      - Bootstrap buttons: 'Previous' (secondary), 'Next' (primary)
      - No keyboard shortcuts for MVP, focus on mouse/touch interaction
    tests:
      - Test interface displays one question at a time
      - Question counter shows correct format 'Question X of Y'
      - Answer options display as radio buttons with clear labels
      - Answer selection saves immediately via AJAX
      - Progress bar updates when questions are answered
      - Previous button navigates to previous question
      - Next button advances to next question
      - Previous button disabled on first question
      - Selected answers persist when navigating between questions
      - AJAX save shows success/error feedback
      - Test session state maintains across page refresh
      - TestAttempt model tracks current question and progress
      - Navigation buttons have appropriate Bootstrap styling
      - Mobile interface works properly on touch devices
      - Answer persistence works without JavaScript enabled
      - Session recovery works after browser crash
    notes:
      - DEPENDENCY MISSING: Need TestAttempt model to track student progress
      - LOGICAL GAP: Where is test session state stored during attempt?
      - FLOW ISSUE: How to handle browser refresh or accidental navigation?
      - REQUIRED: Session persistence mechanism for ongoing tests

  v1.1: Countdown Timer and Auto-Submit on Timeout
    questions:
      - Where should the countdown timer be displayed?
      - What warnings should be shown before timeout?
      - Should there be sound alerts for time warnings?
      - How should auto-submit work?
      - What happens if connection is lost during auto-submit?
      - Should timer be visible on all pages?
    answers:
      - Fixed position at top-right corner of screen
      - Warning alerts at 5 minutes and 1 minute remaining
      - No sound alerts for MVP (browser compatibility issues)
      - Auto-submit via JavaScript form submission when timer reaches zero
      - Show retry message with manual submit button if auto-submit fails
      - Timer visible on all test pages, hidden on result pages
    tests:
      - Countdown timer displays at top-right corner
      - Timer shows correct format (MM:SS)
      - Timer counts down in real-time
      - 5-minute warning alert appears
      - 1-minute warning alert appears
      - Timer turns red in final minute
      - Auto-submit triggers when timer reaches zero
      - Auto-submit works even if user is idle
      - Manual submit button appears if auto-submit fails
      - Timer persists across page navigation
      - Timer resumes correctly after browser refresh
      - Warning alerts don't block test interaction
      - Timer synchronization with server time
      - Multiple warning alerts don't stack
      - Auto-submit prevents further answer changes
      - Timer stops after manual submission

  v1.2: Test Submission and Instant Evaluation
    questions:
      - What confirmation is needed before submission?
      - How should evaluation be calculated?
      - Should partial credit be supported?
      - What happens after submission?
      - How should unanswered questions be handled?
      - Should there be a submission summary?
    answers:
      - Bootstrap modal with 'Are you sure?' and list of unanswered questions
      - Simple scoring: correct answer = 1 point, incorrect/blank = 0 points
      - No partial credit for MVP (only MCQ with single correct answer)
      - Redirect to results page immediately after submission
      - Unanswered questions count as incorrect (0 points)
      - Show summary: 'X out of Y questions answered' before confirmation
    notes:
      - CRITICAL: Store detailed answer data for analytics
      - Each answer should include: was_correct (boolean), answered_at (timestamp)
      - Additional fields: selected_option, time_spent_seconds
      - This data structure supports behavior tracking in v1.5-v1.7
      - Answer model: student, question, test_session, selected_option, is_correct, answered_at
    tests:
      - Submission confirmation modal displays unanswered questions
      - 'Are you sure?' confirmation prevents accidental submission
      - Evaluation calculates correct vs incorrect answers
      - Scoring follows 1 point per correct answer rule
      - Unanswered questions count as incorrect (0 points)
      - Submission summary shows answered question count
      - Instant redirect to results page after submission
      - Answer model stores all required fields correctly
      - Multiple submission attempts are prevented
      - Submission timestamp recorded accurately
      - Partial submissions handle incomplete data
      - Score calculation includes time spent data
      - Results available immediately after submission
      - Submission confirmation shows final score
      - Database transactions maintain data integrity
      - Concurrent submissions handled properly

  v1.2.1: Registration Groups Auto-Creation Bug Fix
    questions:
      - How should missing Django Groups be handled?
      - Should setup_groups command be automated?
      - How should registration be made more robust?
      - What happens when Groups don't exist on fresh clone?
    answers:
      - Auto-create Groups in registration view if they don't exist
      - Use get_or_create() pattern instead of get() to prevent DoesNotExist
      - Make registration process self-healing and setup-independent
      - Gracefully handle fresh repository clones without manual setup
    issue_description:
      - Registration fails with "DoesNotExist at /accounts/register/" error
      - Root cause: Django Groups 'Students' and 'Teachers' missing in fresh clones
      - setup_groups management command exists but isn't run automatically
      - Registration view expects Groups to exist but doesn't create them
    solution:
      - Modify registration view to auto-create Groups using get_or_create
      - Remove dependency on manual setup_groups command execution
      - Ensure registration works immediately after git clone and migrations
      - Use constants from smart_mcq.constants for Group names
    tests:
      - Registration works on fresh repository clone
      - Groups auto-create when they don't exist
      - Registration succeeds when Groups already exist
      - Group creation uses proper constants and names
      - No duplicate Groups created on multiple registrations
      - Profile and Group assignment works correctly

  v1.3: Student Result View (Score + correct vs selected)
    questions:
      - What result information should be shown?
      - How should correct vs incorrect answers be displayed?
      - Should explanations be shown immediately?
      - What charts/graphs should be included?
      - Should there be a detailed question breakdown?
      - Can students review their answers?
    answers:
      - Show total score, percentage, passed/failed status
      - Green checkmark for correct, red X for incorrect answers
      - No explanations for MVP (implement in v2.2)
      - Simple pie chart showing correct vs incorrect distribution
      - Yes, show each question with student's answer vs correct answer
      - Yes, full review of all questions with answers highlighted
    notes:
      - LOGICAL CONFLICT: v1.3 shows correct answers but v2.2 adds explanations
      - DECISION NEEDED: Should correct answers be revealed immediately or delayed?
      - TEACHER CONTROL: Need setting to control when students see correct answers
      - CONSIDERATION: Some teachers may want to review before revealing answers
    tests:
      - Results page displays total score and percentage
      - Pass/fail status shows based on test criteria
      - Correct answers marked with green checkmarks
      - Incorrect answers marked with red X symbols
      - Pie chart displays correct vs incorrect distribution
      - Question breakdown shows student vs correct answers
      - Full question review available with answer highlighting
      - Results page loads quickly with proper formatting
      - Answer comparison clearly distinguishes choices
      - Percentage calculation matches score display
      - Chart.js pie chart renders correctly
      - Student can access results multiple times
      - Results display is mobile-responsive
      - Question images display properly in review
      - Answer choice labels (A,B,C,D) are clear
      - Results cannot be modified by students

  v1.3.1: Persistent Student Result Access
    questions:
      - How should students access completed test results?
      - What URL structure for individual result viewing?
      - Should result access be time-limited or permanent?
      - How should result links be integrated with dashboard?
      - What security measures prevent unauthorized result access?
      - How should result history be organized?
    answers:
      - Students can access any completed test results via dashboard links
      - URL pattern: /accounts/test-results/<attempt_id>/ for individual results
      - Permanent access to all completed tests (no time limits for MVP)
      - Dashboard "View Results" button links to individual result pages
      - Security: Students can only access their own test results via attempt ownership
      - Results organized by completion date on dashboard, newest first
    industry_solution:
      - CANVAS APPROACH: Persistent result access via gradebook with individual submission views
      - BLACKBOARD PATTERN: Direct links from course dashboard to individual test results
      - SECURITY MODEL: URL-based access with ownership validation at view level
      - USER EXPERIENCE: Seamless navigation from dashboard to detailed results
      - MOBILE RESPONSIVE: Result viewing works across all devices consistently
    implementation:
      - Add URL route for individual test result viewing with attempt_id parameter
      - Create result_detail view that fetches TestAttempt and validates ownership
      - Update dashboard template to link "View Results" buttons to individual result URLs
      - Maintain all v1.3 result display functionality with persistent URL access
      - Add security checks to prevent students accessing other students' results
      - Preserve existing result session data for immediate post-submission viewing
    tests:
      - Dashboard "View Results" button links to correct result URL
      - Individual result URL loads correct test results for student
      - Students cannot access other students' results via URL manipulation
      - Result display matches v1.3 formatting and functionality
      - Pie chart and question breakdown work in persistent view
      - Mobile responsive design maintained for result access
      - URL structure follows Django REST patterns for consistency
      - Back to dashboard navigation works from result pages
      - Result access works regardless of browser session state
      - Performance optimized for loading individual results
    notes:
      - RESOLVES UX GAP: Students can now revisit completed test results anytime
      - BRIDGES v0.6 to v2.3: Provides missing persistent result access functionality
      - SECURITY FIRST: Ownership validation prevents unauthorized access
      - MAINTAINS v1.3: All existing result display features preserved
      - PREPARES FOR v1.4.1: Result access control will build on this foundation

  v1.4: Teacher Result View (Student answers & score)
    questions:
      - How should teacher view all student results?
      - What sorting and filtering options are needed?
      - Should there be analytics and statistics?
      - How should individual student performance be shown?
      - What export options are needed?
      - Should there be comparison features?
    answers:
      - Bootstrap table with student name, score, percentage, completion time
      - Basic sorting by name, score, or completion time (no complex filtering)
      - Simple stats: average score, highest/lowest, completion rate
      - Click student name to view detailed answer breakdown
      - No export for MVP (implement in v2.5)
      - No comparison features for MVP, just individual results
    tests:
      - Teacher dashboard shows all student results in table
      - Table columns display name, score, percentage, completion time
      - Sorting works by name, score, and completion time
      - Student name links to detailed answer breakdown
      - Basic statistics show average, highest, lowest scores
      - Completion rate calculation displays correctly
      - Individual student results show all question details
      - Teacher can view which answers student selected
      - Detailed view shows time spent per question
      - Results table paginates for large classes
      - Filter by completion status works
      - Score formatting displays consistently
      - Teacher cannot modify student scores
      - Results update in real-time as students submit
      - Detailed breakdown includes question text
      - Statistics exclude incomplete attempts

  v1.4.1: Result Release Control (Answer Visibility Management)
    questions:
      - How should teachers control when students see results?
      - What answer visibility options should be available?
      - Should there be different settings for practice vs graded tests?
      - How should result release be managed for multiple students?
      - What notification system is needed for result release?
      - How should academic integrity be protected during result release?
    answers:
      - Three-tier result visibility: Score Only → Results with Answers → Enhanced Review
      - Per-test configuration with four release modes: Immediate, Manual, Scheduled, After All Complete
      - Test type setting: Practice (auto-release) vs Assessment (manual review required)
      - Bulk release operations with individual override capabilities
      - Email notifications when results are released to students
      - Answer viewing restrictions and time windows to prevent sharing
    industry_solution:
      - CANVAS SPEEDGRADER MODEL: Results hidden by default, teacher controls release per student or bulk
      - THREE-TIER VISIBILITY: Score → Answers → Feedback progression matches educational best practices
      - ACADEMIC INTEGRITY: Delayed answer release prevents cheating between student attempts
      - FORMATIVE vs SUMMATIVE: Different policies for learning vs assessment contexts
      - NOTIFICATION WORKFLOW: Clear communication when results become available
      - OVERRIDE CAPABILITY: Individual student exceptions while maintaining class policy
      - RELEASE QUEUE: Teacher dashboard showing all pending result releases
    implementation:
      - Test model: result_release_mode field (immediate/manual/scheduled/after_all_complete)
      - Test model: answer_visibility_level field (score_only/with_answers/enhanced_review)
      - Test model: is_practice_test boolean (affects default release behavior)
      - TestAttempt model: result_released_at timestamp, released_by teacher
      - Result release management interface in teacher dashboard
      - Student result view respects release settings and visibility levels
      - Bulk operations for releasing multiple student results
    tests:
      - Test creation form includes result release configuration
      - Practice tests default to immediate release, assessments to manual
      - Teacher can set release mode: immediate/manual/scheduled/after_all_complete
      - Answer visibility levels work: score_only/with_answers/enhanced_review
      - Manual release shows teacher dashboard with pending results queue
      - Bulk release operations work for multiple students simultaneously
      - Individual student result release override functionality
      - Students see appropriate content based on release settings
      - Scheduled release triggers automatically at specified time
      - "After all complete" waits for all students before releasing any results
      - Academic integrity: answer viewing restrictions prevent sharing
      - Result release history tracking for audit purposes
      - Mobile-responsive release management interface
      - Performance optimized for large classes with many results
      - Error handling for failed release operations
    notes:
      - RESOLVES v1.3 CONFLICT: Teachers now control when students see correct answers
      - INDUSTRY STANDARD: Matches Canvas, Blackboard, Moodle best practices
      - PEDAGOGICAL FLEXIBILITY: Supports both formative and summative assessment needs
      - ACADEMIC INTEGRITY: Prevents answer sharing between students taking test at different times
      - TEACHER WORKFLOW: Efficient bulk operations while allowing individual exceptions
    status: COMPLETED

  v1.4.2: UI Enhancement with shadcn/ui Design System Integration
    questions:
      - How should the platform transition from basic Bootstrap to modern SaaS design?
      - What components need updating to follow modern design system patterns?
      - How should shadcn/ui be integrated with existing Bootstrap infrastructure?
      - What consistency rules should be established for future development?
      - How should the component inventory be maintained and tracked?
    answers:
      - Modern SaaS-style UI with clean, professional design following industry standards
      - Complete design system integration using shadcn/ui color tokens and patterns
      - Maintain Bootstrap 5 grid system while adopting shadcn/ui components and styling
      - Comprehensive design system documentation with implementation guidelines
      - YAML-based component tracking with completion status for consistent development
    industry_solution:
      - VERCEL/NEXT.JS APPROACH: shadcn/ui provides modern, accessible component patterns
      - DESIGN TOKENS: HSL-based color system with CSS custom properties for consistent theming
      - TAILWIND INTEGRATION: Utility-first styling with semantic design tokens
      - COMPONENT LIBRARY: Pre-built, accessible components with TypeScript support
      - BOOTSTRAP COMPATIBILITY: Maintain existing grid system while upgrading components
    implementation:
      - Integration of shadcn/ui design tokens (--primary, --border, --radius)
      - Updated navbar with clean, minimal SaaS design (white background, simplified layout)
      - Enhanced card components with proper shadows and hover states
      - Form components with focus-visible states and proper validation styling
      - Comprehensive pagination system across all list pages
      - Design system documentation with implementation guidelines and component patterns
      - Component inventory tracking 255 UI components with completion status
    deliverables:
      - DESIGN_SYSTEM.md: Complete 477-line design system documentation
      - COMPONENT_INVENTORY.md: 398-line comprehensive component catalog
      - COMPONENT_STATUS.yaml: 255 components tracked with completion status
      - Enhanced templates with modern SaaS styling patterns
      - Updated navbar with professional, minimal design
      - Pagination implementation across 6 major list pages
    tests:
      - Design system documentation provides clear implementation guidelines
      - shadcn/ui color tokens integrate properly with existing Bootstrap components
      - Modern card components display with proper shadows and hover effects
      - Form components implement focus-visible states with ring utilities
      - Pagination works consistently across all list pages with query preservation
      - Component inventory accurately tracks all platform components
      - YAML status tracking helps maintain development consistency
      - Mobile responsive design maintained across all updated components
      - Accessibility features preserved with shadcn/ui integration
      - Performance impact minimal with optimized CSS delivery
    notes:
      - DESIGN EVOLUTION: Transitions platform from basic Bootstrap to modern SaaS design standards
      - COMPONENT FOUNDATION: Establishes design system for consistent future development
      - INDUSTRY ALIGNMENT: Adopts proven design patterns from leading SaaS platforms
      - ACCESSIBILITY FIRST: Maintains and enhances accessibility with proper focus states
      - SCALABILITY: Provides foundation for rapid, consistent component development
      - DOCUMENTATION: Creates comprehensive guidelines for design system maintenance

  v1.4.3: Basic Question CSV Import (Teacher Productivity)
    questions:
      - How should teachers import questions via CSV file?
      - What CSV format should be supported?
      - How should validation and error handling work?
      - Should there be a CSV template download?
      - How should duplicate questions be handled?
      - What file size limits are appropriate?
    answers:
      - Simple CSV upload with predefined column structure
      - Standard format: title, description, choice_a, choice_b, choice_c, choice_d, correct_answer, category, difficulty
      - Row-by-row validation with clear error messages and line number references
      - Downloadable CSV template with sample questions and format guidelines
      - Skip duplicates with warning messages, no automatic merging
      - 5MB file size limit, maximum 500 questions per import
    implementation:
      - CSV upload form in question bank with file validation
      - CSV parsing with pandas or Python csv module
      - Batch question creation with transaction rollback on errors
      - Template generation view for CSV format download
      - Import preview showing questions before final import
      - Error reporting with specific line numbers and validation issues
    tests:
      - CSV file upload accepts properly formatted files
      - Template download provides correct format and examples
      - Row validation catches missing required fields
      - Error messages show specific line numbers and issues
      - Import preview displays parsed questions correctly
      - Batch import creates all questions or rolls back on error
      - Duplicate detection works based on title and organization
      - File size validation prevents oversized uploads
      - CSV parsing handles special characters and quotes
      - Import success shows count of questions created
      - Teacher can only import to their own organization
      - Invalid CSV format shows helpful error messages
    notes:
      - PRODUCTIVITY: Addresses teacher workflow efficiency from v0.3 limitation
      - SIMPLE FORMAT: Basic CSV without complex nested data
      - ERROR HANDLING: Clear feedback for troubleshooting import issues
      - BATCH OPERATION: Improves efficiency over manual question entry
      - FOUNDATION: Prepares for advanced bulk operations in v3.11

  v1.5.1: Basic Question Shuffling (SIMPLIFIED)
    questions:
      - How should question order be randomized?
      - Should answer choices be shuffled?
      - How should randomization affect scoring?
      - Should there be seed-based randomization?
    answers:
      - Simple question order randomization per test session
      - Answer choice shuffling per student (A,B,C,D order varies)
      - Scoring remains the same, just question order changes
      - Use student ID + session ID as randomization seed for consistency
    implementation:
      - Add question_order field to TestAttempt (randomized sequence)
      - Add answer_choice_order field to Answer (shuffled A,B,C,D)
      - Generate randomized order when test attempt starts
      - Store shuffle state with student attempt for consistency
    tests:
      - Question order randomizes differently per session
      - Answer choice order shuffles per student attempt
      - Same student gets same order on test resume
      - Scoring works correctly with randomized questions
      - Basic randomization prevents simple answer copying
      - Database schema migration adds new fields without breaking existing data
      - Randomization applies only when enabled per test
      - Answer validation works with shuffled choices
    notes:
      - FOCUS: Essential randomization for academic integrity
      - SIMPLE: No complex pooling or algorithms for MVP
      - CONSISTENT: Same order for student across browser refresh

  v1.5.2: Simple Security Events (SIMPLIFIED)
    questions:
      - What basic security events should be tracked?
      - How should test vs practice modes work?
      - What violation tracking is needed?
      - How should security data be stored?
    answers:
      - Basic event tracking: test start, submit, tab switch detection
      - Add attempt_type field: 'assessment' vs 'practice' (following LMS standards)
      - Simple violation counter in TestAttempt model
      - Store basic security events with timestamps
    implementation:
      - Add attempt_type field to TestSession ('assessment'/'practice')
      - Add violation_count field to TestAttempt
      - Basic JavaScript for tab switch detection
      - Simple event logging without complex analytics
    tests:
      - Tab switch events are detected and logged
      - Assessment vs practice mode saves correctly
      - Violation counter increments on security events
      - Security violation logging includes basic context
      - Event timestamps recorded accurately
      - JavaScript detection works across browsers
      - Security data integrity maintained
      - Basic analytics foundation established
    notes:
      - FOUNDATION: Prepares for advanced behavior tracking in v2.x
      - SIMPLE: Basic detection without complex analysis
      - PRIVACY: Minimal data collection for MVP

  v1.6: Practice Mode Flow Control (SIMPLIFIED)
    questions:
      - How should practice mode differ from assessment mode?
      - What constraints should practice mode have?
      - Should practice mode require correct answers?
    answers:
      - Practice mode: requires correct answer before proceeding to next question
      - Assessment mode: allows free navigation between questions
      - Practice mode: must complete all questions correctly before submission
      - Assessment mode: can submit with unanswered questions
      - Simple UI indicators showing mode and completion requirements
    implementation:
      - Add UI constraints for practice mode navigation
      - Validate practice mode completion before submission
      - Add mode indicators in test interface
      - Simple JavaScript to disable Next button until correct answer
    tests:
      - Practice mode blocks navigation until correct answer selected
      - Practice mode prevents submission until all questions answered correctly
      - Assessment mode allows free navigation and partial submission
      - Mode indicators clearly show current test type to students
      - Practice completion validation works correctly

  v1.7: Basic Result Export (CSV format for MVP)
    questions:
      - What basic export format should teachers have for results?
      - Should exports include basic or detailed data?
      - What simple dashboard enhancements are needed?
    answers:
      - Simple CSV export with student names, scores, completion times
      - Basic export data: no complex analytics, just essential results
      - Enhanced teacher dashboard with basic sorting and filtering
      - Export includes: name, score, percentage, completion time, submission date
      - Simple download button for CSV export per test session
    implementation:
      - Add CSV export functionality to teacher results view
      - Include basic test statistics in export header
      - Simple filtering by completion status (completed/incomplete)
      - Export respects teacher's organization boundaries
    tests:
      - CSV export downloads correctly with proper filename
      - Export includes all completed student results
      - CSV format opens properly in Excel and spreadsheet applications
      - Export filtering works for completed vs incomplete attempts
      - File includes basic test information in header rows
      - Export respects security boundaries (teacher's own tests only)
      - Download works across different browsers
      - Large class exports complete successfully
    notes:
      - SIMPLIFIED FOR MVP: Complex analytics moved to v2.5 Advanced Analytics Dashboard
      - ENCRYPTION: Advanced encrypted exports moved to v2.8 Security Features
      - FOCUS: Essential teacher workflow for result management and basic reporting

  v1.8: Email Templates & Notification System (Foundation)
    questions:
      - What email templates are needed for the platform?
      - How should the notification system be structured?
      - What email events should be supported?
      - How should email templates be organized?
    answers:
      - Basic email templates: verification, password reset, system notifications
      - Simple notification system for essential communications
      - Email events: account verification, password reset, result release (basic)
      - Template organization: Django email templates with base template and inheritance
      - Plain text and HTML email versions for compatibility
    implementation:
      - Django email template system setup
      - Base email template with platform branding
      - Email template inheritance for consistent styling
      - Basic notification queue for email sending
      - SMTP configuration testing and validation
    tests:
      - Email verification template renders correctly
      - Password reset email template contains proper links
      - HTML and plain text versions both work
      - Base template provides consistent styling
      - Email sending works with configured SMTP provider
      - Template inheritance maintains branding consistency
      - Email templates are mobile-responsive
      - Email links generate proper URLs
    notes:
      - FOUNDATION FOR FUTURE: Prepares email infrastructure for v2.4 invitations and v2.7 notifications
      - SIMPLIFIED SCOPE: Focus on essential email functionality only
      - COMPLEX FEATURES MOVED: Advanced analytics moved to v2.5 Advanced Analytics Dashboard

  v1.9: Email Verification & Authentication Security (MOVED UP from v2.3.1)
    questions:
      - What verification process should be used?
      - How should password reset work?
      - What security measures are needed?
      - Should email verification be required or optional?
      - How should rate limiting be implemented?
    answers:
      - Email verification with unique token on registration (REQUIRED for future email features)
      - Password reset via email with secure token link
      - Tokens expire after 24 hours, one-time use only
      - REQUIRED: Email verification before any email-based features work
      - Rate limiting: 3 requests per 15 minutes per email
    industry_solution:
      - GMAIL/OUTLOOK PATTERN: Email verification required before account activation
      - SECURITY STANDARD: Prevents bounce emails and spam complaints
      - DJANGO STANDARD: Built-in email verification with secure token generation
    implementation:
      - Email verification workflow with activation tokens
      - Password reset views and email templates
      - Django email backend configuration (SMTP setup)
      - Security middleware for password policies and rate limiting
      - User profile shows email verification status
      - Email template system for future email features
    tests:
      - Email verification sends unique tokens correctly
      - Password reset generates secure token links
      - Token expiration enforced after 24 hours
      - One-time token usage prevents reuse
      - Django email templates render properly
      - Rate limiting prevents abuse (3 requests per 15 minutes)
      - Expired token handling shows helpful error messages
      - Email verification completes registration process
      - Password reset allows secure password updates
      - SMTP configuration works with major email providers
      - Unverified users cannot access email-dependent features
    notes:
      - CRITICAL FOUNDATION: Required for all future email features (v2.4, v2.7, etc.)
      - SECURITY IMPROVEMENT: Addresses authentication gaps from earlier versions
      - DEPENDENCY RESOLVER: Ensures email invitations in v2.4 work reliably

  v2.0: Question Navigator (Jump to Q1-Q10)
    questions:
      - How should the question navigator be displayed?
      - Should it show answered/unanswered status?
      - How should question numbers be styled?
      - Should there be direct jump functionality?
      - How should it work with question flow?
      - Should there be visual indicators for flagged questions?
    answers:
      - Horizontal button row at top of test interface
      - Yes, answered buttons in green, unanswered in gray
      - Simple numbered buttons (1, 2, 3...) with Bootstrap styling
      - Yes, click any number to jump directly to that question
      - Independent of Next/Previous flow, allows free navigation
      - No flagging indicators for MVP (implement in v2.1)
    tests:
      - Question navigator displays horizontal button row
      - Answered questions show green button styling
      - Unanswered questions show gray button styling
      - Direct jump to question works from any button
      - Navigator position updates with current question
      - Navigation independent of Next/Previous buttons
      - Button numbers display correctly (1, 2, 3...)
      - Navigator responsive on mobile devices
      - Current question highlighted in navigator
      - Bootstrap button styling applies consistently
      - Navigator updates immediately on answer selection
      - Jump navigation preserves answer state
      - Navigator accessible via keyboard navigation
      - Question count matches total test questions
      - Navigator layout doesn't break with many questions
      - Visual feedback clear for answered/unanswered states

  v2.1: Mark for Review Feature
    questions:
      - How should students mark questions for review?
      - Where should review indicators be displayed?
      - How should review list be accessed?
      - Can review marks be toggled?
      - Should there be review reminders?
      - How should review status be saved?
    answers:
      - 'Mark for Review' checkbox below each question
      - Show star icon on question navigator buttons for marked questions
      - 'Review List' button showing all marked questions
      - Yes, checkbox can be toggled on/off
      - Show 'X questions marked for review' reminder before submission
      - Save review status in database with each answer
    tests:
      - 'Mark for Review' checkbox displays below questions
      - Checkbox toggle saves review status immediately
      - Star icon appears on navigator for marked questions
      - 'Review List' button shows all marked questions
      - Review status persists across navigation
      - Submission reminder shows review count
      - Review marks can be toggled on/off
      - Review list updates in real-time
      - Star icons display consistently
      - Review status saves via AJAX
      - Multiple questions can be marked simultaneously
      - Review list accessible from all test pages
      - Review reminder appears before final submission
      - Review marks survive browser refresh
      - Unmarking removes star from navigator
      - Review functionality works on mobile devices

  v2.2: Explanation and Feedback per Question
    questions:
      - How should explanations be structured?
      - Should explanations include multimedia?
      - When should explanations be shown?
      - How should feedback be personalized?
      - Should there be links to additional resources?
      - How should explanation quality be managed?
    answers:
      - Simple text explanation field in Question model
      - No multimedia for MVP, just text explanations
      - Show explanations only after test submission in results view
      - No personalization for MVP, same explanation for all students
      - No external links for MVP, self-contained explanations
      - Teacher responsibility to write quality explanations, no validation
    tests:
      - Explanation field added to Question model
      - Teachers can enter explanations during question creation
      - Explanations display in student results view
      - Explanation text formatting preserved (line breaks)
      - Students see explanations only after submission
      - Explanations show for both correct and incorrect answers
      - No multimedia validation enforced
      - Question edit form includes explanation field
      - Explanation field is optional (not required)
      - Explanations display with proper styling
      - Long explanations format appropriately
      - Teacher can update explanations for existing questions
      - Explanation visibility controlled per question
      - Students cannot edit or modify explanations
      - Explanations enhance learning after test completion
      - Quality improvement depends on teacher input

  v2.3: Student Dashboard (Upcoming, Ongoing, Completed Tests)
    questions:
      - How should different test states be displayed?
      - What information should be shown for each test?
      - Should there be calendar integration?
      - How should notifications be displayed?
      - What quick actions should be available?
      - Should there be achievement/progress tracking?
    answers:
      - Three separate Bootstrap card sections for each state
      - Show test title, date/time, duration, teacher name
      - No calendar integration for MVP, just list view
      - Simple alert banners for notifications (no real-time)
      - Actions: 'Start Test', 'View Results', 'Details'
      - No achievement tracking for MVP, just test history
    tests:
      - Three card sections display for different test states
      - Test information shows title, date, duration, teacher
      - Status indicators use correct colors (blue/green/gray)
      - 'Start Test' button available for ongoing tests
      - 'View Results' button available for completed tests
      - Test cards display chronologically
      - Mobile responsive card layout
      - Test status updates automatically
      - Dashboard loads quickly with multiple tests
      - Alert banners show important notifications
      - Quick actions work from dashboard
      - Test details accessible from cards
      - Dashboard refreshes test status appropriately
      - No calendar integration (list view only)
      - Student cannot access other students' tests
      - Dashboard personalizes content per student

  v2.4: Invite Students via Email or Code (ENHANCED - Now depends on v1.9)
    questions:
      - How should email invitations be sent?
      - What information should be included in invites?
      - How should bulk invitations work?
      - Should there be invitation templates?
      - How should invitation status be tracked?
      - What happens when invitations expire?
    answers:
      - OPTIONAL FEATURE: Email invitations for convenience (teachers can share codes manually via WhatsApp/class)
      - REQUIRES: v1.9 email verification - only verified emails can receive invitations
      - Django's built-in email backend with SMTP configuration (from v1.9)
      - Include test name, date/time, access code, teacher name in email template
      - Simple textarea for multiple verified email addresses (one per line)
      - Single email template for all test invitations
      - Basic delivery tracking (success/failure) since emails are now verified
      - No invitation expiry for MVP, access codes work until test starts
    industry_solution:
      - CANVAS APPROACH: Enrollment-based invitations, not just email blasts
      - STUDENT ROSTER: Teachers manage class rosters with student contact info
      - INVITATION TYPES: Email invite + auto-enrollment, or email with join code
      - TRACKING: Delivery status, open rates, click-through rates
      - REMINDERS: Automatic reminder emails before test deadline
      - IMPLEMENTATION: Invitation model tracks status, scheduled reminder jobs
    notes:
      - DEPENDENCY RESOLVED: v2.3.1 email verification ensures deliverable emails
      - WORKFLOW FLEXIBLE: Teachers can use email invitations OR manual code sharing (WhatsApp, class announcements)
      - NO BOUNCE RISK: Only verified email addresses can receive invitations
      - PROFESSIONAL: Enterprise-grade invitation system with delivery confirmation
      - INTEGRATION: Works seamlessly with enrollment-based access control
    tests:
      - Email invitations send with all required information
      - SMTP configuration works correctly
      - Bulk email sending processes multiple addresses
      - Email template formats properly
      - Access codes included in email body
      - Invalid email addresses handled gracefully
      - Email sending doesn't block UI
      - Invitation emails avoid spam folders
      - Teacher can preview email before sending
      - Email content includes test details clearly
      - Multiple teachers can send invitations
      - Email sending logs for tracking
      - Bounce handling for failed deliveries
      - Email templates maintain professional appearance
      - Invitation workflow integrates with test scheduling
      - Enrollment system provides contact information

  v2.5: Advanced Analytics Dashboard (MOVED FROM v1.7-v1.8)
    questions:
      - How should comprehensive learning analytics be displayed to teachers?
      - What advanced analytics should be calculated from first-click and timing data?
      - How should behavior logs integrate with learning analytics?
      - What dashboard layout provides maximum insight value?
      - How should the analytics match POC capabilities exactly?
      - What performance optimizations are needed for large datasets?
    answers:
      - Comprehensive dashboard: learning efficiency trends, question difficulty heatmaps, student progress analytics
      - POC analytics: fatigue analysis, speed vs accuracy correlation, recovery patterns, random clicking detection
      - Behavior logs: integrate tab switches, focus loss, timing with first-click and learning analytics
      - Dashboard layout: summary cards, interactive charts, detailed student breakdowns, question performance analysis
      - Advanced filtering: by student performance, question difficulty, time ranges, learning efficiency scores
      - Performance: caching analytics calculations, async processing for large exports, database indexing
    poc_feature_completion:
      - Learning efficiency algorithm: (first_attempt_accuracy * 0.7) + (true_learning_rate * 0.3)
      - Speed vs accuracy correlation: Pearson correlation between time spent and accuracy
      - Fatigue analysis: first half vs second half performance comparison
      - Recovery patterns: consecutive wrong answers and recovery tracking
      - Random clicking detection: 3+ attempts indicating guessing behavior
      - Question difficulty classification: Easy (70%+), Medium (50-70%), Hard (30-50%), Very Hard (<30%)
      - Time complexity analysis: relative to average time per question
    tests:
      - Advanced analytics dashboard displays learning efficiency trends
      - Question difficulty heatmaps show performance patterns across questions
      - Student progress analytics track improvement over time
      - Learning efficiency algorithm produces identical results to POC
      - Speed vs accuracy correlation calculates Pearson coefficient correctly
      - Fatigue analysis detects performance degradation accurately
      - Recovery pattern analysis tracks improvement sequences
      - Random clicking detection identifies students with excessive attempts
      - Question difficulty auto-classification matches manual assessment
      - Time complexity analysis provides meaningful insights
      - Dashboard summary cards provide key metrics at a glance
      - Interactive charts allow drill-down into specific data points
      - Student breakdown shows individual learning patterns and efficiency scores
      - Question performance analysis identifies difficult/easy questions automatically
      - Advanced filtering works by student performance levels
      - Time range filtering allows historical analysis
      - Learning efficiency score filtering identifies students needing support
      - Analytics dashboard loads quickly with large datasets
      - Real-time updates don't impact test-taking performance
      - Student insights provide actionable learning recommendations
      - Teacher analytics enable data-driven instruction decisions
      - System scales to hundreds of concurrent users
      - Analytics data integrity maintained across all operations
      - POC feature parity achieved with enhanced Django architecture
    notes:
      - MOVED FROM v1.x: Complex analytics separated from MVP to maintain focus
      - COMPLETE POC INTEGRATION: All proof-of-concept features now available in Django
      - ENHANCED ARCHITECTURE: Better scalability and maintainability than original POC
      - UNIFIED PLATFORM: Single system supports both basic MCQ and advanced analytics
      - PERFORMANCE OPTIMIZED: Enterprise-ready with caching and async processing

  v2.6: CSV/PDF Export of Results (MOVED FROM original v2.5)
    questions:
      - What data should be included in exports?
      - How should PDF reports be formatted?
      - Should there be export templates?
      - What customization options are needed?
      - How should large datasets be handled?
      - Should there be scheduled exports?
    answers:
      - CSV: student name, email, score, percentage, completion time
      - Use ReportLab library for PDF generation (external dependency)
      - Single CSV template and single PDF template
      - No customization for MVP, fixed format exports
      - No special handling for large datasets in MVP
      - No scheduled exports, manual download only
    notes:
      - DEPENDENCY ALERT: ReportLab is first external package beyond Django stack
      - Consider if PDF export is essential for MVP or can be delayed
      - Alternative: Start with CSV only, add PDF in later version
    tests:
      - CSV export includes all specified data fields
      - ReportLab PDF generation produces readable reports
      - Export handles large result datasets
      - CSV format opens correctly in Excel
      - PDF reports maintain consistent formatting
      - Export files download properly
      - File naming convention follows standard pattern
      - Export permissions restricted to teachers
      - Large exports don't timeout
      - Export data accuracy matches dashboard
      - PDF layout remains readable when printed
      - CSV headers clearly identify columns
      - Export includes timestamp for reference
      - Multiple export formats available
      - Export functionality accessible from results page
      - File size reasonable for email sharing

  v2.7: Manual Scoring Support (Code & Subjective Answers)
    questions:
      - How should manual scoring interface work?
      - What grading rubrics should be supported?
      - How should partial credit be assigned?
      - Should there be collaborative grading?
      - How should grading history be tracked?
      - What tools are needed for code evaluation?
    answers:
      - Simple form with student answer display and score input field
      - No rubrics for MVP, teacher assigns points manually
      - Allow decimal scores (0.5 out of 1.0) for partial credit
      - No collaborative grading for MVP, single teacher scores
      - Store original score, modified score, and modification timestamp
      - No code evaluation tools for MVP, teacher reviews code manually
    tests:
      - Manual scoring interface displays student answers clearly
      - Teachers can assign decimal scores (0.5 out of 1.0)
      - Score input validation prevents invalid values
      - Partial credit calculations work correctly
      - Grading history tracks original and modified scores
      - Modification timestamps recorded for audit
      - Score changes update total test score
      - Manual grading accessible from teacher dashboard
      - Multiple teachers can grade same test
      - Grading interface shows question context
      - Code answers display with syntax highlighting
      - Subjective answers show full text responses
      - Grading workflow maintains data integrity
      - Students notified when manual grading complete
      - Rubric suggestions provided for consistency
      - Bulk grading operations for efficiency

  v2.7: Basic Email Notifications (Test Schedule, Result Alerts)
    questions:
      - What email templates are needed?
      - How should notification preferences be managed?
      - What triggers should send notifications?
      - Should there be digest emails?
      - How should email delivery be tracked?
      - What customization options for emails?
    answers:
      - Templates: test invitation, test reminder, results available
      - Simple checkbox in user profile: 'Receive email notifications'
      - Triggers: test scheduled, 1 hour before test, results published
      - No digest emails for MVP, individual notifications only
      - No delivery tracking for MVP, assume emails are delivered
      - No customization for MVP, fixed email templates
    notes:
      - CONFLICT: Email reminders need scheduling mechanism
      - v2.7 introduces timed tasks but v3.6 introduces async/Redis
      - RESOLUTION: Use simple cron job or django-crontab for v2.7
      - Migrate to Celery/Redis when implementing v3.6
    tests:
      - Email notification preferences save correctly
      - Test invitation emails trigger on scheduling
      - Reminder emails send 1 hour before test
      - Results notification emails sent after grading
      - Email templates render with proper formatting
      - Notification preferences prevent unwanted emails
      - Cron job scheduling works reliably
      - Email delivery doesn't fail silently
      - Multiple notification types can be configured
      - Email content personalizes per recipient
      - Notification timing respects timezone settings
      - Email sending integrates with user preferences
      - Digest functionality works if implemented
      - Email customization maintains professional tone
      - Notification system scales with user volume
      - Email scheduling handles server downtime

  v2.8: Encrypted Result Export (Advanced Security Features)
    questions:
      - What export format should match the POC encrypted results?
      - How should encrypted exports preserve all analytics data?
      - What encryption method provides adequate security?
      - How should encrypted files be delivered to teachers?
    answers:
      - Export encrypted .enc files using AES encryption with same format as POC (JSON with analytics data)
      - Encrypted exports: include all first-click data, learning efficiency, timing, behavior, and performance analytics
      - AES-256 encryption with secure key derivation from teacher credentials
      - Simple download links with encrypted file delivery (no email attachment for security)
    implementation:
      - AES encryption library integration for secure exports
      - POC-compatible .enc file format with JSON structure
      - Complete analytics data serialization for export
      - Secure key management for encryption/decryption
      - Teacher dashboard export controls and file management
    tests:
      - Encrypted .enc export generates files matching POC format exactly
      - AES encryption preserves all analytics data in exports
      - Encrypted exports include all first-click data, learning efficiency, timing
      - Export files decrypt correctly with proper credentials
      - Secure key derivation prevents unauthorized access
      - File download security maintains teacher boundaries
      - Analytics data integrity maintained in encrypted format
      - Large encrypted exports complete successfully
      - Encryption performance acceptable for production use
    notes:
      - MOVED FROM v1.7: Advanced security features separated from MVP
      - POC COMPATIBILITY: Maintains exact format compatibility with proof-of-concept
      - SECURITY ENHANCED: Production-grade encryption implementation
      - TEACHER WORKFLOW: Seamless integration with existing export functionality

  v2.9: Test vs Practice Flow Control (REVISED)
    questions:
      - What are the key flow differences between test and practice modes?
      - How should practice mode enforce learning constraints?
      - How should test mode allow assessment flexibility?
      - What navigation restrictions apply to each mode?
      - How should completion validation differ between modes?
      - What analytics apply to both modes equally?
    answers:
      - Test mode: free navigation between questions, can submit incomplete, assessment-focused
      - Practice mode: must answer correctly before proceeding, cannot submit until 100% complete, learning-focused
      - Radio button selection when creating test session (mode affects UI flow only)
      - Test mode: all navigation enabled, can skip questions, submission allowed anytime
      - Practice mode: Next button disabled until correct answer, Submit button disabled until all correct
      - SAME analytics for both: first-click tracking, learning efficiency, timing, behavior monitoring
    industry_solution:
      - CANVAS APPROACH: Practice attempts vs Graded attempts
      - ATTEMPT LIMIT: Set max attempts per student (e.g., 3 practice, 1 graded)
      - SCORING POLICY: Highest score, latest score, or average score
      - TIME LIMITS: Different time limits for practice vs graded
      - FEEDBACK: Immediate feedback in practice, delayed in graded
      - IMPLEMENTATION: attempt_type field, max_attempts setting per test
    notes:
      - LOGIC RESOLVED: Multiple attempts = multiple TestAttempt records
      - STANDARD: Industry approach separates attempt types, not modes
      - FLEXIBILITY: Teachers control attempt limits and scoring policies
    tests:
      - Test mode allows free navigation between all questions
      - Test mode permits submission with incomplete answers
      - Practice mode blocks Next button until correct answer selected
      - Practice mode blocks Submit button until all questions answered correctly
      - Mode selection during test session creation works correctly
      - Practice mode UI clearly indicates completion requirements
      - Test mode shows normal navigation without restrictions
      - Both modes capture identical analytics data (first-click, timing, behavior)
      - Learning efficiency calculations work identically in both modes
      - Security monitoring applies equally to both modes
      - Question shuffling works consistently across both modes
      - Answer choice shuffling applies to both modes equally
      - Teacher dashboard shows same analytics regardless of mode
      - Student results display same detailed analytics for both modes
      - Encrypted exports include complete data for both test and practice attempts
      - Mode switching preserves all analytics and data integrity

  v2.9: Class-Based Test Assignment (Post-MVP Enhancement)
    questions:
      - How should classes be created and managed?
      - How should students join classes?
      - How should test assignment to classes work?
      - Should there be bulk student management?
      - How should class enrollment be tracked?
      - What happens to existing access code workflow?
    answers:
      - Simple class creation with name, description, and auto-generated join code
      - Students join via 8-digit class code (similar to current test access codes)
      - Teachers assign test sessions to entire classes with one click
      - Basic CSV import for bulk student enrollment
      - Track enrollment date and status per student-class relationship
      - Access codes remain as fallback option alongside class assignments
    industry_solution:
      - GOOGLE CLASSROOM MODEL: Simple class codes, stream-based assignments
      - CANVAS PATTERN: Course enrollment with assignment distribution
      - HYBRID APPROACH: Both class assignment and direct access codes supported
    implementation:
      - Class model with teacher ownership and join codes
      - ClassEnrollment model for student-class relationships
      - TestAssignment model linking test sessions to classes
      - Enhanced student dashboard showing "My Classes" and "Assigned Tests"
      - Teacher class management interface with enrollment tracking
      - Assignment workflow integrated into existing test session creation
    tests:
      - Teachers can create classes with auto-generated join codes
      - Students can join classes using 8-digit class codes
      - Teachers can assign test sessions to entire classes
      - Students see assigned tests prominently on dashboard
      - Class enrollment tracking works correctly
      - Bulk student import via CSV functions properly
      - Access code workflow continues to work alongside assignments
      - Class-assigned tests take priority over general available tests
      - Teacher can view class enrollment and assignment status
      - Class management integrates smoothly with existing UI
    notes:
      - SOLVES: Student test discovery problem identified in v0.6
      - MAINTAINS: Existing access code workflow for backward compatibility
      - PRIORITY: Post-MVP feature after core test-taking functionality complete
      - SIMPLICITY: Focuses on class-based assignment only, not full LMS complexity

  v3.0: Coding Question Type (Text-Based, No Execution)
    questions:
      - What code editor features are needed?
      - How should coding questions be structured?
      - What programming languages to support?
      - How should answers be submitted and evaluated?
      - Should there be code templates?
    answers:
      - Basic textarea with syntax highlighting (CodeMirror for display only)
      - Text-based coding questions with manual review
      - Python, Java, JavaScript question templates
      - Students submit code as text, teachers review manually
      - Code templates and starter code provided per question
      - No automatic execution - manual grading workflow
    implementation:
      - CodeMirror editor for syntax highlighting (read-only execution)
      - Question templates for different programming languages
      - Large text area for student code submission
      - Manual grading interface for teachers
      - Code comparison tools for plagiarism detection
    notes:
      - SIMPLIFIED APPROACH: No Docker dependency, manual evaluation only
      - DOCKER EXECUTION: Moved to v3.5 Code Execution Sandbox
      - MVP FOCUS: Code questions without execution complexity
      - MANUAL WORKFLOW: Teachers grade coding questions like essays
    tests:
      - CodeMirror editor loads with syntax highlighting
      - Python syntax highlighting works correctly
      - Test case definition interface accepts JSON format
      - Code execution sandbox prevents system access
      - Time and memory limits enforced during execution
      - Function template provides starting code structure
      - Test case results display pass/fail status
      - Code submission saves student work
      - Execution errors display helpful messages
      - Docker containerization isolates code execution
      - Multiple test cases run sequentially
      - Code editor supports basic editing features
      - Execution results integrate with scoring system
      - Security measures prevent malicious code
      - Performance monitoring tracks execution metrics
      - Fallback to manual review if execution fails

  v3.1: Fill in the Blanks / Short Answer Type
    questions:
      - How should blanks be defined in questions?
      - What input types should be supported?
      - How should answers be validated?
      - Should there be auto-completion features?
      - How should multiple blanks be handled?
      - What scoring methods for partial credit?
    answers:
      - Use placeholder syntax like [BLANK1] in question text
      - Text inputs and select dropdowns
      - Exact string matching (case-insensitive)
      - No auto-completion for MVP
      - Each blank scored separately, average for final score
      - Simple partial credit: correct blanks / total blanks
    tests:
      - Blank placeholders [BLANK1] parse correctly in questions
      - Text input fields generate for each blank
      - Select dropdown options display for choice blanks
      - Answer validation performs case-insensitive matching
      - Multiple blanks scored independently
      - Partial credit calculation averages blank scores
      - Blank numbering displays clearly to students
      - Fill-in answers save automatically
      - Question preview shows blank placement
      - Teacher can define acceptable answer variations
      - Short answer validation handles whitespace
      - Blank types (text/select) configure per question
      - Scoring shows which blanks correct/incorrect
      - Mobile interface works for fill-in questions
      - Answer review displays student vs correct responses
      - Bulk blank creation tools for efficiency

  v3.2: Matching and Ordering Question Types
    questions:
      - How should matching interfaces work?
      - What drag-and-drop functionality is needed?
      - How should ordering questions be presented?
      - Should there be visual feedback?
      - How should complex matching be handled?
      - What accessibility considerations?
    answers:
      - Simple drag-and-drop with HTML5 drag API
      - Basic drag items to target zones
      - Sortable list with drag handles
      - Visual feedback with CSS hover states
      - Keep matching simple, 1-to-1 relationships only
      - Ensure keyboard navigation works for accessibility
    tests:
      - Drag and drop functionality works smoothly
      - HTML5 drag API supports all modern browsers
      - Matching items snap to target zones clearly
      - Sortable lists respond to drag handles
      - Visual feedback shows valid drop zones
      - CSS hover states provide clear interaction cues
      - Keyboard navigation works for accessibility
      - Touch interface supports mobile drag/drop
      - Matching validation ensures 1-to-1 relationships
      - Order verification works for sequencing questions
      - Drag operations don't interfere with page scrolling
      - Answer state saves during drag operations
      - Visual indicators show successful matches
      - Error handling for invalid drag operations
      - Matching interface scales with question complexity
      - Alternative input methods for accessibility compliance

  v3.3: Question Randomization
    questions:
      - How should randomization be configured?
      - Should there be question pools?
      - How should randomization affect scoring?
      - Should there be seed-based randomization?
      - How should fairness be ensured?
      - What reporting for randomized tests?
    answers:
      - Simple checkbox 'Randomize questions' when creating test
      - No complex pools, random selection from all test questions
      - Scoring remains the same, just question order changes
      - No seed-based randomization, simple random shuffle
      - Ensure all students get same questions, just different order
      - Show original question order in teacher reports
    industry_solution:
      - BLACKBOARD APPROACH: Store randomized order per student attempt
      - NAVIGATOR FIX: Show 'Question 1 of 10' instead of actual question numbers
      - SEED-BASED: Use student ID + attempt ID as randomization seed for consistency
      - QUESTION MAPPING: Store question_order field in TestAttempt
      - ANALYTICS: Reports show both original and randomized order
      - IMPLEMENTATION: Generate and store question sequence when attempt starts
    notes:
      - CONFLICT RESOLVED: Navigator shows position, not original question number
      - CONSISTENCY: Same student gets same order on resume
      - ANALYTICS: Full traceability for teacher review
    tests:
      - Question randomization checkbox toggles correctly
      - Random question order generates per student
      - Scoring remains consistent across randomized tests
      - Question navigator shows position not original number
      - Student gets same order on test resume
      - Seed-based randomization ensures consistency
      - Teacher reports show both original and randomized order
      - Question mapping stores in TestAttempt model
      - Randomization applies only when checkbox enabled
      - Analytics track question performance regardless of order
      - All students receive same questions in different order
      - Randomization doesn't affect answer choices
      - Question sequence generation works reliably
      - Performance analytics account for randomization
      - Teacher review shows student-specific question order
      - Randomization enhances test security effectively

  v3.4: Answer Shuffling
    questions:
      - How should answer options be shuffled?
      - Should shuffling be configurable per question?
      - How should this affect answer keys?
      - Should there be consistent shuffling?
      - How should special answer types be handled?
      - What impact on analytics?
    answers:
      - Random shuffle of A, B, C, D options for each student
      - Apply to all questions, no per-question configuration
      - Store original correct answer, map to shuffled position
      - Different shuffle for each student
      - No special handling, all MCQ options treated equally
      - Analytics show original answer labels for consistency
    tests:
      - Answer options shuffle randomly for each student
      - Original correct answer maps to shuffled position
      - All MCQ options shuffle equally (A, B, C, D)
      - Different students get different shuffle patterns
      - Analytics display original answer labels consistently
      - Shuffle configuration applies to all questions
      - Answer key accuracy maintained through shuffling
      - Student sees shuffled options in test interface
      - Teacher reports show original option labels
      - Shuffle state saves with student attempt
      - Answer validation works with shuffled options
      - Performance analytics normalize to original labels
      - Shuffling doesn't affect question text or images
      - Multiple choice scoring remains accurate
      - Review interface shows student's shuffled view
      - Answer shuffling enhances academic integrity

  v3.5: Code Execution Sandbox (Docker Integration)
    questions:
      - How should code execution be sandboxed securely?
      - What Docker configuration provides adequate isolation?
      - How should test cases be executed automatically?
      - What security measures prevent malicious code?
      - How should execution timeouts and resource limits work?
      - What error handling and feedback should be provided?
    answers:
      - Docker containers for complete isolation per code execution
      - Lightweight Python Docker image with restricted permissions
      - Automatic test case execution with input/output comparison
      - Network isolation, filesystem restrictions, resource limits
      - 30-second timeout, 128MB memory limit, no network access
      - Detailed error messages for compilation and runtime errors
    implementation:
      - Docker integration with secure container configuration
      - Test case execution engine with automatic grading
      - Resource monitoring and limitation enforcement
      - Secure code execution API with proper error handling
      - Integration with existing coding question interface from v3.0
    tests:
      - Docker containers execute student code securely
      - Test cases run automatically with proper input/output matching
      - Resource limits prevent system overload
      - Network isolation prevents external access
      - Timeout handling prevents infinite loops
      - Error messages help students debug their code
      - Code execution results integrate with grading system
      - Multiple simultaneous executions handled correctly
      - Container cleanup prevents resource leaks
      - Security sandbox prevents privilege escalation
    notes:
      - MOVED FROM v3.0: Separated execution complexity from basic coding questions
      - MAJOR DEPENDENCY: First introduction of Docker to the platform
      - SECURITY CRITICAL: Proper sandboxing essential for safety
      - PERFORMANCE IMPACT: Container startup time affects user experience
      - SCALABILITY: Consider resource usage for large classes

  v3.6: Question Pooling (Tag-Based Dynamic Test)
    questions:
      - How should question pools be defined?
      - What tag system should be used?
      - How should pool selection work?
      - Should there be pool weighting?
      - How should pool exhaustion be handled?
      - What pool analytics are needed?
    answers:
      - Create pools based on question categories/tags
      - Simple tag CharField, comma-separated tags
      - Random selection from each pool based on requirements
      - Equal weighting for MVP, no complex algorithms
      - Show warning if pool has insufficient questions
      - Basic analytics: questions per pool, usage statistics
    tests:
      - Question pools defined by category/tag system
      - Tag-based selection creates dynamic tests
      - Pool selection algorithm chooses randomly from pools
      - Pool exhaustion warnings display when insufficient questions
      - Equal weighting distributes questions fairly across pools
      - Pool analytics show usage and performance statistics
      - Tag system supports comma-separated multiple tags
      - Dynamic test generation varies by student
      - Pool requirements specification interface for teachers
      - Question categorization supports pool creation
      - Pool validation ensures minimum question counts
      - Analytics track question effectiveness by pool
      - Dynamic tests maintain consistent difficulty
      - Pool management interface intuitive for teachers
      - Tag-based filtering works in question bank
      - Pool strategy balances coverage and randomization

  v3.6: Redis + Async Handling (High Load, Event Queue)
    questions:
      - What data should be cached in Redis?
      - How should session data be managed?
      - What async tasks need queuing?
      - How should cache invalidation work?
      - What monitoring for Redis performance?
      - How should failover be handled?
    answers:
      - Cache: question data, user sessions, test results
      - Store active test sessions in Redis for fast access
      - Queue: email sending, result calculations, file exports
      - Simple TTL-based expiration, manual invalidation when needed
      - Basic monitoring: connection status, memory usage
      - Fallback to database if Redis unavailable
    tests:
      - Redis connection establishes successfully
      - Question data caches with appropriate TTL
      - User session data stores in Redis efficiently
      - Test result caching improves response times
      - Cache invalidation triggers on data changes
      - Email queue processing works via Celery
      - Result calculation tasks execute asynchronously
      - File export operations queue properly
      - Redis monitoring shows connection and memory status
      - Graceful fallback to database when Redis down
      - Cache hit rates improve application performance
      - Session persistence works across server restarts
      - Async task status tracking functions correctly
      - Cache warming strategies load popular data
      - Redis clustering supports high availability
      - Performance metrics validate caching effectiveness

  v3.7: Batch Code Execution (Queued, Sandboxed)
    questions:
      - How should code execution be queued?
      - What sandboxing technology to use?
      - How should resource limits be enforced?
      - What security measures are needed?
      - How should execution results be returned?
      - What monitoring for execution systems?
    answers:
      - Use Celery with Redis as message broker
      - Docker containers for sandboxing
      - CPU time limits, memory limits, disk space limits
      - Network isolation, read-only file system
      - Store results in database, notify via WebSocket
      - Monitor queue length, execution times, failure rates
    tests:
      - Code execution queues via Celery successfully
      - Docker containers provide secure sandboxing
      - CPU time limits prevent infinite loops
      - Memory limits constrain resource usage
      - Disk space limits prevent storage abuse
      - Network isolation blocks external connections
      - Read-only file system prevents file tampering
      - Execution results store in database correctly
      - WebSocket notifications inform of completion
      - Queue monitoring tracks performance metrics
      - Failed executions handled gracefully
      - Execution timeout prevents hanging processes
      - Resource monitoring prevents system overload
      - Batch processing handles multiple submissions
      - Security measures validated against common attacks
      - Scalable execution supports concurrent users

  v3.8: Question & Option Caching
    questions:
      - What caching strategies should be used?
      - How should cache keys be structured?
      - What cache invalidation triggers?
      - How should cache performance be monitored?
      - What fallback mechanisms?
      - How should cache warming work?
    answers:
      - Cache frequently accessed questions and test data
      - Keys: 'question:{id}', 'test:{id}:questions', 'user:{id}:tests'
      - Invalidate when questions/tests are modified
      - Monitor cache hit rates, response times
      - Fall back to database queries if cache misses
      - Pre-load popular questions and active tests
    tests:
      - Question caching improves load times significantly
      - Cache keys follow consistent naming convention
      - Cache invalidation triggers on question updates
      - Test data caching reduces database queries
      - User-specific cache entries partition correctly
      - Cache performance monitoring shows hit rates
      - Fallback queries work when cache misses
      - Cache warming loads frequently accessed data
      - Memory usage stays within acceptable limits
      - Cache expiration policies prevent stale data
      - Popular content identification works accurately
      - Active test detection triggers appropriate caching
      - Cache efficiency metrics guide optimization
      - Multi-level caching strategy balances performance
      - Cache coherency maintained across application servers
      - Performance improvements measurable and significant

  v3.9: User Session Caching (Fast resume/load)
    questions:
      - What session data should be cached?
      - How should session restoration work?
      - What happens on cache misses?
      - How should concurrent sessions be handled?
      - What session cleanup procedures?
      - How should session security be maintained?
    answers:
      - Cache: current question, answers, time remaining, session state
      - Restore from cache when user returns to test
      - Rebuild session from database if cache miss
      - Allow single active session per user per test
      - Cleanup expired sessions daily
      - Encrypt sensitive session data, validate session tokens
    tests:
      - Session data caches current test state reliably
      - Answer progress persists across browser refresh
      - Time remaining calculation maintains accuracy
      - Session restoration works after disconnection
      - Cache miss rebuilds session from database
      - Single active session limit enforced per user
      - Expired session cleanup runs automatically
      - Session encryption protects sensitive data
      - Session token validation prevents hijacking
      - Concurrent session handling prevents conflicts
      - Session state includes all critical test data
      - Fast session loading improves user experience
      - Session security measures prevent tampering
      - Daily cleanup removes old session data
      - Session persistence survives server restarts
      - Session recovery maintains test integrity

  v3.10: Enhanced Authentication & User Management (MVP Gap Fix)
    questions:
      - How should password reset functionality work?
      - Should email verification be required for registration?
      - How should user profile management be implemented?
      - What admin capabilities are needed for user management?
      - How should account security be enhanced?
    answers:
      - Django's built-in password reset with email links
      - Email verification required before account activation
      - User profile page with password change, info updates
      - Admin interface for user activation/deactivation, role changes
      - Password strength requirements, account lockout after failed attempts
    implementation:
      - Password reset views and email templates
      - Email verification workflow with activation tokens
      - User profile forms and update views
      - Enhanced admin interface with user management tools
      - Security middleware for password policies and lockout
    notes:
      - SOLVES: Authentication gaps identified in v0.2 analysis
      - SECURITY: Addresses open registration security concerns
      - USABILITY: Provides essential user account management features

  v3.11: Question Bank Enhancement (Advanced Bulk Operations & Search)
    questions:
      - What advanced search and filtering capabilities are needed?
      - How should question versioning be implemented?
      - Should there be question sharing between teachers?
      - What analytics are needed for question usage?
      - How should bulk edit/delete operations work?
    answers:
      - Advanced search by title/category/difficulty, filter by usage/date/performance
      - Question history tracking with restore previous versions capability
      - Question sharing within organization with permission controls and approval workflow
      - Usage analytics showing question performance, difficulty analysis, and effectiveness metrics
      - Bulk edit operations for category/difficulty changes, bulk delete with confirmation
    implementation:
      - Advanced search and filter interface with multiple criteria
      - Question versioning model with change history and diff viewing
      - Sharing permissions and collaboration features with approval workflow
      - Analytics dashboard for question insights and performance metrics
      - Bulk operations interface with preview and confirmation steps
    tests:
      - Advanced search works with multiple filter combinations
      - Question versioning tracks all changes with timestamps
      - Question sharing workflow includes approval process
      - Analytics dashboard shows meaningful performance insights
      - Bulk operations complete successfully with rollback on errors
      - Search performance remains fast with large question banks
      - Version history shows clear change differences
      - Sharing permissions respect organization boundaries
    notes:
      - BUILDS ON: v1.4.3 CSV import foundation for basic bulk operations
      - EFFICIENCY: Enables large-scale question management for enterprise use
      - COLLABORATION: Supports teacher question sharing with proper governance
      - ANALYTICS: Provides data-driven insights for question quality improvement

  v3.12: Test Bank Advanced Features (Templates & Randomization)
    questions:
      - How should test templates and cloning work?
      - What randomization options should be available?
      - How should passing scores be configured?
      - What attempt limits and retake policies?
      - Should there be test preview functionality?
    answers:
      - Test templates with reusable structures, one-click cloning
      - Question randomization, choice order randomization
      - Configurable passing score percentage with grade boundaries
      - Attempt limits per student with retake policies
      - Test preview mode for teachers to review before publishing
    implementation:
      - Template system with test structure definitions
      - Randomization engine for questions and choices
      - Scoring configuration with grade calculation
      - Attempt tracking with policy enforcement
      - Preview mode with teacher review interface
    notes:
      - SOLVES: Test management gaps from v0.4 analysis
      - FLEXIBILITY: Provides advanced test configuration options
      - QUALITY: Ensures test integrity through preview system

  v3.13: Organization & Multi-Tenancy Enhancement
    questions:
      - How should organization admin roles work?
      - What organization-specific settings are needed?
      - How should multi-organization data isolation work?
      - What organization management tools are required?
      - How should organization billing/subscription work?
    answers:
      - Organization admin role with user/teacher management rights
      - Custom branding, test policies, user registration controls
      - Complete data isolation with organization-scoped queries
      - Organization dashboard with usage statistics and controls
      - Basic subscription tracking (full billing in v4.2)
    implementation:
      - Organization admin role and permissions
      - Organization settings model and configuration interface
      - Enhanced data filtering for multi-tenancy
      - Organization management dashboard
      - Basic subscription status tracking
    notes:
      - RESOLVES v0.1 INCONSISTENCY: v0.1 claimed "multi-tenant ready from day one" but only basic Organization model was implemented
      - CLARIFICATION: v0.1-v1.4.1 had basic organization field for future-proofing, not true multi-tenancy
      - REAL MULTI-TENANCY: v3.13 implements actual data isolation and organization management
      - ENTERPRISE: Prepares for B2B multi-tenant deployment
      - SCALABILITY: Enables proper organization isolation
      - ARCHITECTURE EVOLUTION: Single-tenant MVP → Multi-tenant enterprise platform

  v3.14: Student Experience Enhancement (Progress & History)
    questions:
      - How should student progress tracking work?
      - What test history and portfolio features?
      - Should there be practice mode for tests?
      - What study materials and prep features?
      - How should student performance analytics work?
    answers:
      - Progress tracking with completion rates and improvement trends
      - Test history with detailed results and score progression
      - Practice mode with unlimited attempts and instant feedback
      - Study materials section with downloadable resources
      - Student analytics dashboard with performance insights
    implementation:
      - Progress tracking models and calculation engine
      - Test history interface with detailed views
      - Practice mode with separate scoring system
      - Study materials management and distribution
      - Student analytics with charts and trends
    notes:
      - SOLVES: Student experience gaps identified in MVP analysis
      - ENGAGEMENT: Improves student motivation through progress tracking
      - PREPARATION: Provides tools for better test preparation

  v3.15: Teacher Tools & Analytics Enhancement
    questions:
      - What advanced analytics should teachers have?
      - How should gradebook functionality work?
      - What reporting and export capabilities?
      - Should there be communication tools?
      - What bulk management features for classes?
    answers:
      - Detailed analytics on test performance, question difficulty, student progress
      - Gradebook with score tracking, grade calculations, manual adjustments
      - Comprehensive reports exportable to PDF/Excel with custom filters
      - Basic messaging system for teacher-student communication
      - Bulk operations for student management, grade import/export
    implementation:
      - Advanced analytics engine with multiple visualization options
      - Gradebook interface with grade management tools
      - Report generation system with customizable templates
      - Simple messaging system with notifications
      - Bulk management tools for efficient class administration
    notes:
      - SOLVES: Teacher workflow gaps from v0.2-v0.5 analysis
      - EFFICIENCY: Provides comprehensive class management tools
      - INSIGHTS: Enables data-driven teaching decisions

  v4.0: B2B Pilot Launch for Colleges
    questions:
      - What onboarding process is needed?
      - How should pilot feedback be collected?
      - What success metrics should be tracked?
      - What support documentation is needed?
      - How should pilot limitations be communicated?
      - What graduation criteria from pilot?
    answers:
      - Guided setup process with admin account creation
      - Feedback forms, regular check-ins, usage analytics
      - Track: user adoption, test completion rates, performance
      - User guides, video tutorials, FAQ documentation
      - Clear pilot terms, feature limitations, support boundaries
      - Criteria: stable usage, positive feedback, payment commitment
    tests:
      - Onboarding process creates admin accounts successfully
      - Guided setup walks through essential configurations
      - Feedback collection forms capture user input
      - Usage analytics track adoption and engagement
      - Test completion rates monitored accurately
      - Performance metrics show system reliability
      - User guides accessible and comprehensive
      - Video tutorials explain key features clearly
      - FAQ addresses common pilot questions
      - Pilot limitations clearly communicated to users
      - Support boundaries defined and maintained
      - Success metrics tracked against graduation criteria
      - Stable usage patterns demonstrate product value
      - Positive feedback indicates user satisfaction
      - Payment commitment process works smoothly
      - Pilot management dashboard shows key metrics

  v4.1: Multi-Organization Support (User & Data Isolation)
    questions:
      - How should organizations be structured?
      - What data isolation mechanisms are needed?
      - How should user management work across orgs?
      - What admin capabilities per organization?
      - How should billing be handled?
      - What migration path from single-org?
    answers:
      - Organization model with hierarchical structure
      - Row-level security, organization-scoped queries
      - Users belong to organizations, role-based access
      - Org admins manage users, tests, settings within org
      - Per-organization billing, usage tracking
      - Migration script to assign existing data to default org
    industry_solution:
      - SALESFORCE APPROACH: Multi-tenant architecture with shared schema
      - ORG MEMBERSHIP: Users can belong to multiple organizations via membership table
      - PERMISSION MODEL: Role-based permissions scoped to organizations
      - DATA ISOLATION: All queries filtered by organization context automatically
      - BILLING: Per-organization usage tracking with aggregated billing
      - ADMIN HIERARCHY: System Admin → Org Admin → Teachers → Students
      - IMPLEMENTATION: Use django-tenant-schemas or custom middleware
    notes:
      - RESOLVED: Built from v0.1 if organization field added to all models
      - INDUSTRY PATTERN: Proven multi-tenant SaaS architecture
      - SCALABILITY: Supports enterprise-level organization management
    tests:
      - Organization model supports hierarchical structure
      - Row-level security isolates organization data
      - User management scoped to organization context
      - Role-based access controls organization boundaries
      - Per-organization billing tracking functions
      - Data migration assigns existing data to default org
      - Multi-tenant queries filter by organization automatically
      - Organization membership supports multiple affiliations
      - Permission model scales to large organizations
      - System admin can manage all organizations
      - Org admin permissions limited to their organization
      - Data isolation prevents cross-organization leaks
      - Billing aggregation works across organization units
      - Admin hierarchy enforces proper access levels
      - Django middleware handles organization context
      - Tenant schema isolation provides data security

  v4.2: White-labeled Branding (Logo, Theme, Domain)
    questions:
      - What branding elements can be customized?
      - How should theme management work?
      - What domain configuration options?
      - How should assets be managed?
      - What preview capabilities are needed?
      - How should branding inheritance work?
    answers:
      - Logo, colors, fonts, custom CSS, email templates
      - Theme settings per organization, CSS variable overrides
      - Custom domain mapping, SSL certificate management
      - File storage for logos, upload management interface
      - Live preview of theme changes before applying
      - Organization themes override global defaults
    tests:
      - Logo upload and display works correctly
      - Color customization applies throughout interface
      - Font selection affects all text elements
      - Custom CSS overrides default styling
      - Email template branding matches organization theme
      - Theme settings save and load per organization
      - Custom domain mapping functions properly
      - SSL certificate management works automatically
      - Asset management handles file uploads securely
      - Live preview shows changes before applying
      - Theme inheritance follows organization hierarchy
      - Branding consistency maintained across features
      - White-label appearance removes platform branding
      - Mobile responsive design preserves custom styling
      - Theme performance doesn't impact load times
      - Brand asset optimization maintains quality

  v4.3: Admin Usage and Audit Logs
    questions:
      - What admin actions should be logged?
      - How should audit trails be displayed?
      - What search and filter capabilities?
      - How should log retention be managed?
      - What compliance features are needed?
      - How should log export work?
    answers:
      - Log: user actions, data changes, admin operations, logins
      - Chronological table with action, user, timestamp, details
      - Search by user, action type, date range
      - Configurable retention periods, automatic cleanup
      - Immutable logs, digital signatures for compliance
      - Export to CSV, PDF for audit purposes
    tests:
      - User action logging captures all admin operations
      - Data change tracking maintains complete audit trail
      - Login attempt logging includes success and failures
      - Audit log display shows chronological entries
      - Search functionality finds specific audit events
      - Date range filtering works accurately
      - User-based filtering shows individual activity
      - Action type filtering groups similar events
      - Configurable retention policies auto-cleanup old logs
      - Immutable log entries prevent tampering
      - Digital signatures ensure log integrity
      - CSV export includes all audit fields
      - PDF export formats professionally
      - Log storage scales with organization size
      - Compliance features meet regulatory requirements
      - Audit interface intuitive for administrators

  v4.4: Internal Messaging (Student ↔ Teacher Chat)
    questions:
      - What messaging features are needed?
      - How should conversations be organized?
      - Should there be group messaging?
      - What notification preferences?
      - How should message history be managed?
      - What moderation capabilities?
    answers:
      - Basic text messaging, file attachments, read receipts
      - Organize by test/course, conversation threads
      - Group messaging for class announcements
      - Email notifications, in-app notifications
      - Persistent message history, search capabilities
      - Message reporting, content filtering, admin oversight
    tests:
      - Basic text messaging works between students and teachers
      - File attachment upload and download functions
      - Read receipts show message delivery status
      - Conversation organization by test/course context
      - Thread management keeps discussions organized
      - Group messaging reaches all class participants
      - Email notifications send for new messages
      - In-app notifications appear in real-time
      - Message history search finds past conversations
      - Persistent storage maintains conversation continuity
      - Message reporting system flags inappropriate content
      - Content filtering blocks problematic messages
      - Admin oversight provides moderation capabilities
      - Mobile interface supports messaging features
      - Message privacy respects organization policies
      - Bulk messaging tools help with announcements

  v4.5: Advanced Email Features & Delivery Analytics (ENHANCED from original)
    questions:
      - How should email delivery analytics be implemented?
      - What advanced email customization is needed?
      - How should email templates be managed per organization?
      - What additional security measures are needed?
      - How should bulk email processing be optimized?
    answers:
      - Email delivery tracking with open rates, click tracking, bounce handling
      - Organization-specific email templates and branding customization
      - Advanced template editor with preview functionality
      - Enhanced security with IP restriction and advanced rate limiting
      - Queue-based email processing with retry logic and failure handling
    notes:
      - BUILDS ON: v2.3.1 email foundation and v2.4 invitation system
      - ENTERPRISE FEATURES: Advanced analytics and customization for large organizations
      - PERFORMANCE: Optimized email handling for high-volume deployment

# ARCHITECTURE SUMMARY
# This specification follows industry-proven patterns from major LMS platforms

core_architecture:
  technology_stack:
    - Django + PostgreSQL + Bootstrap (until v3.6)
    - Redis + Celery (v3.6+)
    - Multi-tenant ready from v0.1
  
  data_models:
    - User → Profile → Enrollment → Course/Class → Test → TestAttempt → Answer
    - Organization field in all models for multi-tenancy
    - TestAttempt as central state management
  
  access_control:
    - Enrollment-based discovery (primary)
    - Access code fallback (secondary)
    - Role-based permissions (Students/Teachers)
    - Teacher controls result release timing
  
  scalability_features:
    - Multi-organization support
    - Session state persistence
    - Violation management policies
    - Question randomization
    - Caching and async processing

development_principles:
  - MVP first, enterprise features later
  - Industry-standard solutions over custom
  - Proven LMS patterns (Canvas/Blackboard/Moodle)
  - Django+PostgreSQL+Bootstrap core stack
  - External dependencies only when necessary

key_differentiators:
  - Behavior tracking and proctoring
  - Flexible attempt management
  - Industrial-grade session recovery
  - Multi-tenant architecture from day one
  - Professional invitation and enrollment system